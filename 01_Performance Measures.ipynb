{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Performance Measures\n",
    "\n",
    "* [1 - Regression Problems](#regression)\n",
    "* [2 - Classification Problems](#classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"regression\">\n",
    "\n",
    "## 1. Regression Problems\n",
    "</a>\n",
    "\n",
    "* [1.1. - $R^{2}$ Score](#rsquare)\n",
    "* [1.2. - Adjusted $R^{2}$ Score](#adjusted)\n",
    "* [1.3. - MAE](#mae)\n",
    "* [1.4. - RMSE](#mse)\n",
    "* [1.5. - MedAE](#medae)\n",
    "\n",
    "Import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1`__ Import the dataset __Boston.csv__ and define as data the independent variables and target the dependent variable (last column) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv(r'./Datasets/Boston.csv')\n",
    "data_boston = boston.iloc[:,:-1]\n",
    "target_boston = boston.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2`__ By using the method train_test_split from sklearn.model_selection, split your dataset into train(80%) and validation(20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_boston, \n",
    "                                                    target_boston, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=15, \n",
    "                                                    shuffle=True, \n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3`__ Create an instance of LinearRegression named as lr with the default parameters and fit to your train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4`__ Now that you have your model created, assign the predictions to y_pred, using the method predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5`__ From __slearn.metrics__ import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"rsquare\">\n",
    "    \n",
    "### 1.1. $R^{2}$ Score\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score'>sklearn.metrics.r2_score(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "R^2 (coefficient of determination) regression score function.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). \n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 6`__ Check the R^2 score of the model you created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When to use?__ <br>\n",
    "When we want to measure the amount of variance in the target variable that can be explained by our model. <br>\n",
    "It gives the degree of variability in the target variable that is explained by the model or the independent variables. <br>\n",
    "If this value is 0.7, then it means that the independent variables explain 70% of the variation in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"adjusted\">\n",
    "    \n",
    "### 1.2. Adjusted $R^{2}$ Score\n",
    "\n",
    "</a>\n",
    "\n",
    "There is no direct way to obtain the adjusted R^2 using sklearn, but we can apply the formula:\n",
    "<img src=\"adj_r2.png\" alt=\"Drawing\" style=\"width: 300px;\"/> <br>\n",
    "\n",
    "\n",
    "where n stands for the sample size and p for the number of the regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 7`__ Calculate the Adjusted R^2 Score for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "n = len(y_val)\n",
    "p = len(X_train.columns)\n",
    "\n",
    "def adj_r2 (r2,n,p):\n",
    "    return 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "adj_r2(r2,n,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When to use?__ <br>\n",
    "When we want to measure the amount of variance in the target variable that can be explained by our model. <br>\n",
    "This is a form of R-squared that is adjusted for the number of terms in the model. <br>\n",
    "Tries to avoid the problem associated with R-squared:  even if we are adding redundant variables to the data, the value of R-squared does not decrease - it either remains the same or increases with the addition of new independent variables.\n",
    "\n",
    "__Then what is the advantage of $R^{2}$?__ <br>\n",
    "It has a direct interpretation as the proportion of variance in the dependent variable that is accounted for by the model.\n",
    "\n",
    "\n",
    "<hline>\n",
    "\n",
    "***\n",
    "    \n",
    "However in some cases we are more interested in quantifying the error in the same measuring unit of the variable:\n",
    "    - we can use metrics like MAE, MSE and MedAE for that.\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"mae\">\n",
    "    \n",
    "### 1.3. MAE (Mean absolute error)\n",
    "\n",
    "</a>\n",
    "\n",
    "<img src=\"mae.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "__`Step 8`__ Check the MAE of the model you created previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error'>sklearn.metrics.mean_absolute_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Mean absolute error regression loss.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible value is 0.0. MAE is always non-negative.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When to use?__ <br>\n",
    "It measures the average magnitude of the errors in a set of predictions, without considering their direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"mse\">\n",
    "    \n",
    "### 1.4. RMSE (Root Mean squared error)\n",
    "\n",
    "</a>\n",
    "\n",
    "<img src=\"rmse.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "<!-- __`Step 9`__ Check the RMSE of the model you created previously -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error'>sklearn.metrics.mean_squared_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Mean absolute error regression loss.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible value is 0.0. MSE is always non-negative.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_val, y_pred, squared = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When to use?__ <br>\n",
    "Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable.\n",
    "\n",
    "__MAE vs. RMSE__ <br>\n",
    "RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 20 is more than twice as bad as being off by 10. But if being off by 20 is just twice as bad as being off by 10, then MAE is more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"medae\">\n",
    "    \n",
    "### 1.5. MedAE (Median absolute error)\n",
    "\n",
    "</a>\n",
    "\n",
    "__`Step 10`__ Check the MedAE score of the model you created previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.median_absolute_error'>sklearn.metrics.median_absolute_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Median absolute error regression loss\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible value is 0.0. MedAE is always non-negative.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_absolute_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When to use?__ <br>\n",
    "Using the median instead of the mean implies that we are ignoring the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"classification\">\n",
    "\n",
    "## 2. Classification Problems\n",
    "</a>\n",
    "\n",
    "* [2.1. - The confusion matrix](#confusion)\n",
    "* [2.2. - The accuracy Score](#accuracy)\n",
    "* [2.3. - The precision](#precision)\n",
    "* [2.4. - The recall](#recall)\n",
    "* [2.5. - The F1 Score](#f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 11`__ Import the needed libraries to apply logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 12`__ Import the dataset __final_tugas.csv__ and define the independent variables as __data__ and the dependent variable ('DepVar') as __target__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tugas = pd.read_csv(r'./Datasets/final_tugas.csv')\n",
    "data_tugas = tugas.drop(['DepVar'], axis=1)\n",
    "target_tugas = tugas['DepVar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 13`__ By using the method train_test_split from sklearn.model_selection, split your dataset into train(80%) and validation(20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data_tugas, \n",
    "                                                  target_tugas, \n",
    "                                                  test_size = 0.2, \n",
    "                                                  random_state=5, \n",
    "                                                  stratify = target_tugas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 14`__ Create an instanve of LogisticRegression named as __log_model__ with the default parameters and fit to your train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 15`__ Now that you have your model created, assign the predictions to y_pred, using the method predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "y_pred = log_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 16`__ From __slearn.metrics__ import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics used for classification differ from the ones used for regression. <br>The sklearn library offers a wide range of metrics for this situation. We are going to see the most used ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"confusion\">\n",
    "    \n",
    "### 2.1. The confusion matrix\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix'>sklearn.metrics.confusion_matrix(y_true, y_pred, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute confusion matrix to evaluate the accuracy of a classification\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 17`__ Obtain the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix in sklearn is presented in the following format: <br>\n",
    "[ [ TN  FP  ] <br>\n",
    "    [ FN  TP ] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"accuracy\">\n",
    "    \n",
    "### 2.2. The accuracy score\n",
    "\n",
    "</a>\n",
    "\n",
    "<img src=\"accuracy.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score'>sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True,...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Accuracy classification score.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "If normalize is True, then the best performance is 1. When normalize = False, then the best performance is the number of samples.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "_normalize_: If False, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples. <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 18`__ Get the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is accuracy always a good option? Let's check with an example:\n",
    "\n",
    "<img src=\"example_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "In this case, what is the accuracy?\n",
    "\n",
    "<img src=\"example_2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "We have an accuracy of 99,1% which is very very high! That is great, right? <br>\n",
    "Well, not really...<br>\n",
    "Imagine that we are testing people potentially with covid... A positive person is actually someone who is sick and carrying a virus that can spread very quickly! The cost of having a misclassified actual positive (or a false negative) is very high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"precision\">\n",
    "    \n",
    "### 2.3. The precision\n",
    "\n",
    "</a>\n",
    "\n",
    "<img src=\"precision.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score'>sklearn.metrics.precision_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute the precision.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "The best value is 1, and the worst value is 0.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 19`__ Get the precision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the confusion matrix, we can verify that precision is only concerned to the predicted values that were considered positive:\n",
    "    \n",
    "<img src=\"example_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "So precision gives us how precise / accurate our model is out of those predicted positive, how many of them are actual positive.\n",
    "\n",
    "__When to use?__\n",
    "\n",
    "`When the cost of False Positives is high.` <br>\n",
    "For example, in email spam detection, where a negative is considered not spam and a positive is a spam email. <br>\n",
    "A false positive will be an email that is considered spam when in reality it was not - the user will loose potentially importante information if the precision is not high in the spam detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"recall\">\n",
    "    \n",
    "### 2.4. The recall\n",
    "\n",
    "</a>\n",
    "<img src=\"recall.png\" alt=\"Drawing\" style=\"width: 180px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.recall_score'>sklearn.metrics.recall_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute the recall.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "The best value is 1 and the worst value is 0.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 20`__ Get the recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrix:\n",
    "    \n",
    "<img src=\"example_4.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Recall calculates how many of the actual positives our model is able to capture through labeling it as positive (True positive).\n",
    "\n",
    "__When to use?__\n",
    "\n",
    "`When the cost of False Negatives is high.` <br>\n",
    "For example, in the example we gave before concerning Covid tests. If a sick patient (Actual Positive) does the test and is predicted as not sick (predicted as negative), the risk will be extremely high since the sickness is contagious. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"f1\">\n",
    "    \n",
    "### 2.5. The F1 Score\n",
    "\n",
    "</a>\n",
    "\n",
    "<img src=\"f1.png\" alt=\"Drawing\" style=\"width: 270px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score'>sklearn.metrics.f1_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "F1 score reaches its best value at 1 and worst score at 0.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 21`__ Get the F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When to use?__\n",
    "\n",
    "F1 Score should be used when you want to seek a balance between Precision and Recall and if there is an uneven class distribution (large number of Actual Negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 22`__ To evaluate the results, we are going to use also the classification report method. <br>\n",
    "Import __classification_report__ from __sklearn.metrics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 23`__ Create  a function named `metrics` that will print the results of the classification report and the confusion matrix for both datasets (train and validation) _(written for you)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_train, pred_train , y_val, pred_val):\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                     TRAIN                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_train, pred_train))\n",
    "    print(confusion_matrix(y_train, pred_train))\n",
    "\n",
    "\n",
    "    print('___________________________________________________________________________________________________________')\n",
    "    print('                                                VALIDATION                                                 ')\n",
    "    print('-----------------------------------------------------------------------------------------------------------')\n",
    "    print(classification_report(y_val, pred_val))\n",
    "    print(confusion_matrix(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 24`__ Create an object named __labels_train__ that will containt the predicted values for the train and another one named __labels_val__ that will contain the predicted values for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = log_model.predict(X_train)\n",
    "labels_val = log_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 25`__ Call the function metrics() defined previously, and define the arguments: <br> (`y_train = y_train`, `pred_train = labels_train` , `y_val = y_val`, `pred_val = labels_val`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "metrics(y_train = y_train, pred_train = labels_train, y_val = y_val, pred_val = labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: <br>\n",
    "https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d <br>\n",
    "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
