{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Selection\n",
    "\n",
    "In this notebook, we are going to apply different ways to split your dataset and try to select the best model.\n",
    "\n",
    "__`Step 1`__ Import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2`__ Read the dataset __tugas.xlsx__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tugas = pd.read_excel(r'./Datasets/tugas.xlsx')\n",
    "tugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3`__ Create an object named __data__ that will contain your independent variables and another object named __target__ that will contain your independent varaible / target (the last column in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "data = tugas.iloc[:,:-1]\n",
    "target = tugas.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. The train-test split\n",
    "\n",
    "In this approach we randomly split the complete data into training and test sets. Then Perform the model training on the training set and use the test set for validation purpose, ideally split the data into 70:30 or 80:20. With this approach there is a possibility of high bias if we have limited data, because we would miss some information about the data which we have not used for training. If our data is huge and our test sample and train sample has the same distribution then this approach is acceptable.\n",
    "\n",
    "In this exercise, we are going to split our dataset into train, test and validation. <br> <br>\n",
    "By default, sklearn has a function named train_test_split that allows to split the dataset into two different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4`__ Import the library `train_test_split` from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5`__ Divide the `data`into `X_train_val` and `X_test`, the `target`into `y_train_val` and `y_test`, and define the following arguments: `test_size = 0.2`, `random_state = 15`, `shuffle = True` and `stratify = target` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(data, \n",
    "                                                    target, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=15, \n",
    "                                                    shuffle=True, \n",
    "                                                    stratify=target\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will allow me to create two different datasets, one for train (80% of the data) and one for test (20% of the data). <br>\n",
    "The stratification will allow me to have the same proportion of each label of the dependent variable in both datasets.\n",
    "\n",
    "\n",
    "### How to create the three datasets: train, validation and test?\n",
    "To create three datasets (train, validation and test) we are going to use the function train_test_split twice. <br><br>\n",
    "First we are going to create two sets of datasets, one for test (X_test and y_test) and another one that includes the data for training and validation (X_train_val and y_train_val).\n",
    "\n",
    "__`Step 6`__  Divide the `X_train_val`into `X_train` and `X_val`, the `y_train_val` into `y_train` and `y_val`, and define the following arguments: `test_size = 0.25`, `random_state = 15`, `shuffle = True` and `stratify = y_train_val`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val,\n",
    "                                                  y_train_val,\n",
    "                                                  test_size = 0.25,\n",
    "                                                  random_state = 15,\n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=y_train_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 7`__ Check the proportion of data for each dataset. _(written for you)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train:{}% | validation:{}% | test:{}%'.format(round(len(y_train)/len(target),2),\n",
    "                                                     round(len(y_val)/len(target),2),\n",
    "                                                     round(len(y_test)/len(target),2)\n",
    "                                                    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three different datasets, namely:\n",
    "- Training dataset, with 60% of the data, that will allow me to build the model;\n",
    "- Validation dataset, with 20% of the data, that will allow me to fine tune the model and check some problems like overfitting;\n",
    "- Test dataset, with 20% of the data, that will allow me to evaluate the performance of the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different techniques we are going to check in this step are commonly used in applied machine learning to compare and select a model for a given predictive modeling problem.\n",
    "\n",
    "In the following cases, we are going to check the performance of a Logistic Regression using those different techniques.\n",
    "\n",
    "In the following examples we are going to use the dataset `Diabetes.csv`\n",
    "\n",
    "__`Step 8`__ Read the dataset __Diabetes.csv__ and define the independent variables as X and the dependent variable as y (the last column in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'./Datasets/Diabetes.csv')\n",
    "# DO IT\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 9`__ Import __KFold__ from __sklearn.model_selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 10`__ Import __LogisticRegression__ from __sklearn.linear_model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 11` Create a function named as __run_model_LR__ that receives as parameters the dependent variable and the independent variables and returns a fitted Logistic Regression model to the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "def run_model_LR(X,y):\n",
    "    model = LogisticRegression().fit(X,y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 12`__ Create a function named as __evaluate_model__ that receives as parameters the independent variables, the dependent variable and the model and returns the score method result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X,y, model):\n",
    "    return model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 13`__ Create a function named __avg_score_LR__ that will return the average score value for the train and the test set. This will have as parameters the technique you are going to use, your dependent variable and your independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score_LR(method,X,y):\n",
    "    score_train = []\n",
    "    score_test = []\n",
    "    for train_index, test_index in method.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model = run_model_LR(X_train, y_train)\n",
    "        value_train = evaluate_model(X_train, y_train, model)\n",
    "        value_test = evaluate_model(X_test,y_test, model)\n",
    "        score_train.append(value_train)\n",
    "        score_test.append(value_test)\n",
    "\n",
    "    print('Train:', np.mean(score_train))\n",
    "    print('Test:', np.mean(score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 14`__ Create a KFold Instance where the number of splits is 10 (*n_splits*) and name it as __kf__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 15`__ Call the function __avg_score_LR__ and check the average score for the train and the test sets using __kf__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_LR(kf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Repeated K-Fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 16`__ Import __RepeatedKFold__ from __sklearn.model_selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 17`__ Create a RepeatedKFold Instance where the number of splits is 6 (`n_splits=6`) and the number of times cross-validator needs to be repeated is 2 (`n_repeats=2`)  and name it as __rkf__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "rkf = RepeatedKFold(n_splits=6, n_repeats=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 18`__ Call the function __avg_score_LR__ and check the average score for the train and the test sets using __rkf__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "avg_score_LR(rkf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Leave One Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 19`__ Do the same steps you applied on the previous techniques, but this time using the Leave One Out. For that, you need to import __LeaveOneOut__ from __sklearn.model_selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "avg_score_LR(loo, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Stratified k-fold and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SkLearn you have several options to select your model, and the application is similar to the cases we saw previously.\n",
    "\n",
    "<img src=\"model_selection.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models\n",
    "\n",
    "Don't forget that the purpose of this notebook is to compare different models. In this step, you are going to fit your data into a DecisionTree model also, and use the __RepeatedKFold__ to compare the performance of it with the Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 21`__ Import __DecisionTreeClassifier__ from __sklearn.tree__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 22`__ Similarly to Step 11, create a function named as __run_model_DT__ that receives as parameters the dependent variable and the independent variables and returns a fitted Decision Tree Classifier model to the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "def run_model_DT(X,y):\n",
    "    model = DecisionTreeClassifier().fit(X,y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 23`__ Similarly to step 13, create a function named __avg_score_DT__ that will return the average score value for the train and the test set. This will have as parameters the technique you are going to use, your dependent variable and your independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "def avg_score_DT(method,X,y):\n",
    "    score_train = []\n",
    "    score_test = []\n",
    "    for train_index, test_index in method.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        model = run_model_DT(X_train, y_train)\n",
    "        value_train = evaluate_model(X_train, y_train, model)\n",
    "        value_test = evaluate_model(X_test,y_test, model)\n",
    "        score_train.append(value_train)\n",
    "        score_test.append(value_test)\n",
    "\n",
    "    print('Train:', np.mean(score_train))\n",
    "    print('Test:', np.mean(score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 24`__ Apply RepeatedKFold to the data using `n_splits = 6` and `n_repeats = 2` and check the performance of the DecisionTree you created by calling the function __avg_score_DT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "rkf2 = RepeatedKFold(n_splits=6, n_repeats=2)\n",
    "avg_score_DT(rkf2, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
